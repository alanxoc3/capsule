:cd     -- change directory
:exit   -- exit shell
:err    -- only capture stderr
:out    -- only capture stdout
:nil    -- ignore stdout
:ret    -- get return codes of previous command
:pre    -- run before every command. output goes to stdout
:post   -- filter after every command
:key    -- called after a keypress only for interactive mode
:funcs  -- list the stored functions
:str    -- turns statements into strings

# these don't have to be in the shell, if the shell writes files for {} syntax
# but that would be a huge performance hit
:loop   -- loop on condition
:if     -- conditionally execute statements
:fork   -- run commands in parallel & wait for all to finish

http://www.pixelbeat.org/programming/stdio_buffering/
https://www.gnu.org/software/coreutils/manual/coreutils.html#stdbuf-invocation
http://www.linusakesson.net/programming/tty/

$var local var
%var exported env var
:builtin

Core concepts:

    array  - args
    keyval - env vars
    stdin  - input  (sequential)
    stdout - output (sequential)
    stderr - error  (sequential)
    fildes - pipes

Buffered (configure buffer) & unbuffered

Buffered (configure buffer)

:bg
cat | :fork (awk  | $3 param)
            (sed  | $2 param)
            (grep | $1 param)
:fg

Are arguments an array? Or are they a configuration language?

$3

$1 (?) # set var to empty string
$1 ""  # set var to empty string

[$1]


lines starting with spaces mean a continuation of the previous command

have a configuration that specifies which file descriptors are bound to the shell. input and output file descriptors.

:pipebuf 0 # in bytes, set the size of the pipe buffer

|< :min
|> :blah

| 1 0 2 3 4 5

cat file |1 :split (:fd 2 | 01) (:fd 1 | 31) (:fd 0)


cat file | $pi | awk | :bg

cat meow | awk

awk | $pi





1 | 01 | 01 |

:take 1
:give

Pipe types:
buffer
no buffer

files  - storage/system

new thoughts:
- {} is more of a string.
- () and [] are the same
should it evaluate at all?

()

a echo {some string}

"" ''

{} ` ~~

Special chars: # | () []
string chars: ` {} "" ''
command chars: $ % :

awk {{print "blah"}}

awk (? {{print "blah"}} | tr ' ' '\0')
awk '

awk {{print "blah"}}

i wonder if the "~" multiline syntax is even needed then? Because {} is just soo useful.

also, there should be a hook for multiline strings

:aoseutnh invalid

:$ list of env vars
:% list of local vars

env vars are a hashmap.kk

for the most part, just the first character of the first command has special meaning.

:: that should have a meaning

| is the pipe operator

:fork ~(echo what) (echo hi)
echo what > cat < echo hi

code is in strings.

kak has this %{} string thing, which is pretty useful.

a shortcut that adds to the ax_path might be cool.

:a

builtins can control evaluation

[] is just a string

if one of the brackets were a string instead, that would be nice. Like what kakoune does.

()

[:first]
`( )

[]

:if [echo hi] [
    :if []
] []



printf ~~
#!/bin/bash
what is this?
%s
~~ thing


~~ blah

() - current shell

%() - string

:set var val

Some good defaults:
:pre ? '> '

:post :buff '
' {:out} {:err} {:nil | $? | :ret}

pipe will control stdout, err, and ret code

or maybe only capture 1 file descriptor?
blah | (cat)

ignore stderr?

pipe ignores stderr?
stderr and ret get sent to file?
:err command # prints stderr
:out command # prints stdout

axu err command
axu out command
axu ret command
axu get out,err,ret command

variables are buffers?

:set x=ret,y=err {command}
"$" is like tee for variables
command | :err | $x
command | :fork {$y} # discards output
$y # prints val of variable, or keeps printing until pipe is closed

{} is command syntax. just saved as an in-memory script.

command | :wait # keeps output?

echo {echo hello} /tmp/ax-pid-uuid ~/.cache/ax/pid/aoeuaboen

{echo hello}   # prints hello
(? '' '') # tries running empty string
[? '' ''] # tries running empty string with argument
(? echo) hello # prints hello
[? echo hello] # prints hello

? {echo hello}        # prints: :aseontuhsaoenbusnth:
:aseontuhsaoenbusnth: # prints hello

OK, I actually like the reverse pipe.
$lvar | grep he | awk // | :serial {?' ' hello world this is me} {echo hello} {echo bye} {echo hellew}

:save | :builtins

# print a new number every second
while {sleep 1s; $i | trim | bc | ? ($i)+1} {echo ($i) whatever ($2)}

tee file

:fork {a} {b} {c} | 
:wait
:serial {a} {b} {c}
:if {a} {b} {c}

[{blah $0 $1 $2 $3} {heh} {yo}]
command not found: 'blah'

there is no going back.

builtins are special, because they can access internal state

%evar | $lvar
%evar [$lvar]

echo hi ; echo mwahahaha
sed s|blah|blah|g


:script {echo whatup}
:fork (random {blah} {blow})

err by default 

? [echo some command]

:pre {? '> '} called before you type input. similar to the idea of "$PS1" in other shells. defaults to "{? '> '}" for interactive scripts. defaults to "{}" for non interactive scripts.

:post {:get err,out}
  called right before each command is executed in the current shell session. imagine this command being appended as the last pipe to the command about to be executed.


{} evaluating to a command works for hooks... maybe...

:exec   -- executes a string # is this needed?
in the ax shell there is a concept of things coming out of a program.
namely: ret code, stderr, stdout

other things come out of programs, but these are the only important ones for the shell.

by default, nothing comes out of a program. you need to use the :hook command to make something come out.

# default hook:

 #  -- rest of line is ignored
 ;  -- separates statements
' ' -- string between quotes
" " -- string between quotes

pipe vs channel

| :

( ) -- unpack first    (exactly 1)
[ ] -- unpack all      (1 or more)
{ } -- unpack nonempty (0 or more)

 |  -- pipes output between expressions
 :  -- prefix for builtins
 ?  -- prefix for joining
 $  -- prefix for local variables
 %  -- prefix for environment variables
 ~  -- string between ~XYZ ... XYZ~

mv {mv ($2) (%HOME)/.bin/($1)} (%HOME)/.bin/a
a e {echo [$@]}
a l {ls [$@]}
a gs {git status [$@]}

ax  - blah
axu - axutils

{echo hello ($1) ($PATH | axu range 3-4 :)} what

{echo hello ($1) ($PATH | axu range 3-4 :)} what

{scope}
:command { some command } | tee file.txt
(:str {echo [$@]} | axu bin blah) hello "" "" "" it's a wonderful world
:exec (:str {echo [$@]})

"what?"

(? 'echo ($1)' | axu script) what

:if {script} {script}

:for () {}

[%HOME]/.bin/e

awk '{print $1}'

evaluation order.
echo {? $1 $2 $3} (blah blah blah)

;cyclic pipe example:
    |: :stream [echo hi apple; grep hi] | grep apple |:

order of named pipe matters

|& grep h; |& cat; echo hi |& # both cat and grep will run
echo hi |&; |& grep h; |& cat # grep will print, but cat won't

echo what |err # 
echo what |out # 
echo what |all # group stdout and stderr
echo what |nil # point to /dev/null

|- |err $var1 (nothing)
|- |out $var2
echo what |all |-

if pipe input comes before output, only the next pipe will work
if pipe output comes before input, nothing is printed until you get input

  echo hi |
| what

| hey
| you
  yoyo ma |
| testing


# what does this mean?
| cat    | what | buffer
| grep h | what | buffer

|out | tee out
|err | tee err
cat file |

# if statement starts with pipe, don't execute until you find a statement that ends with a pipe.
| :o | cat
| :e | ? [while y [cat file]] |.
|. cat |

# don't execute until you find statement
# or execute but buffer

cat file |
| blah

# take input from nowhere and feed it into cat
| cat

# put the output nowhere
cat file |
| cat |
| cat file.txt - |

# means nothing, because no way to feed into anonymous pipe
# actually means run this command as usual (for most commands)
| cat

# means nothing, because no way to read from anonymous pipe
# actually, means run this command silently.
cat | 

# setup listener for b pipe. write to listener of b pipe.
|b cat
echo :ret
cat blah |b

cat blah | cat

:wait {cat blah | awk | grep | sed | tac | :nil; ?' ' [:ret]} {other process}
:spawn


:fork # spawns threads and continues with program.          output is not displayed in terminal.
:wait # spawns processes and waits for all of them to exit. output is     displayed in terminal.


:bg {:fork {} {} {}}

mv {echo $1} first

:fork {} {} {}

:help :bg

:{? $2} arg1 arg2
{? $2} arg1 arg2

:script x {radico8} {what does this mean?}

:blah blah blah

:for 1 {radico8}

function puts things on your path

# prints "arg2"


xargs vs expansion

~ blah ~

:{}
:()
:[]
||

{echo 



# all commands are run in parallel unless attached to the current process.
# only one command can be attached to the current process at a time

# how do i know to continue to next line or not?


| sed
| grep |
cat f |

|a a |k
|a b |k
| c |a

|a d |k
|a e |k
| f |a

g |
| f
| c

    d
  f + e
g +
  c + a
    b

by default, stderr gets printed and stdout goes into command

printing to stdout is a blocking operation no shell prompt when printing to stdout

But if you setup a pipe away from stdout, that's the same as running a separate process

and should there be support for file descriptors? idk. i also don't know if i would use that.

cat f | grep | sed

| nice

  cat file |
| grep hi  |
| sed what |
| sponge   |
| tee file




| yes
| no

defaults to "|all" if there is no pipe afterwards. or "|out" if no pipe before.

:err {
|:err

echo 

:if {} {} {} {} {}
:if [? x y z]
:if (! a b c)

:ret 

oh no, are there 2 data types? executables and strings.

$a {.echo ($1)} 
.mv ($a) (basename ($a))/first
?' ' what is that; echo

$a 'hey, what up'

($a) 

cat file |aa

cat |   # pipe cat output to anonymous pipe, so stop taking input when the pipe buffer is full

cat |aa # pipe cat to named pipe, stop taking input until pipe is read from

cat file |. grep hi |. grep what |.

:set |. 33
|0
|{name, buf size}

how is a pipe buffered. what is the buffer size on a pipe.

:fork {grep hello} {grep what}

|1 grep --line-buffered |3
|2 grep --line-buffered |3

cat [fd -0 hello]

[ fd -0 hello | tr '\0\0|^\0|\0$' ]

:if {}

{echo ($0) ($1)} hello
    /tmp/bin/aoesntha hello

:if (true) {echo hello} {}

:if (:fd 14 true)


? [grep]

[ ] -- evaluates as strings
`~        not multiline
`#        not comment
`:command not builtin

Here is how pipexec does multiple file descriptors:
    $ pipexec -- [ A /usr/bin/cmd1 ] [ B /usr/bin/cmd2 ] "{A:1>B:0}" "{B:1>A:0}"

|1 |3

|p1 grep hi |p1 apple

|p1 awk '{print $1}'

name processes
:a | :b | :c

utf8 is supported, because it has no null characters has prefix of 1.

does the ax shell even need any notion of utf8? probably not. that's something that can be left up to the sub programs.

Actually, ax needs to support the notion of utf8. For multiline strings.

I need anonymous named pipes.
The ability to have a circular pipe is needed.
Controlling buffer size of pipe is needed.
https://github.com/flonatel/pipexec

# issues and fixes for ax shell proposal
A few ideas:
- Probably shouldn't do the reverse pipe thing.
- A string type that works with nested parenthesis would be useful maybe.
- If and Loop maybe should actually work by parsing strings.

the "?" builtin can get past the argument length limit. ax can also get past
env var limits. but when you run a program, you might run into the limit.

how would ax react with a large ~ mmultiline ultilinemultiline smultiline tringmultiline ~ ?
if going into a program argument, the kernel would complain.
if going into the "?" operator, works as expected.

Pipe max on system is equal to max file descriptor: /proc/sys/fs/file-max (insanely high)

Env vars and arguments both have limits. Stdin/stdout does not.

how to start somewhere in a script?

"HEOLL WORDU"
'ease asouhn'

execute string as script. if could be offloaded into separate program. 

:exec [if (test -n ($hello)] 'text if true' 'command if false')
:if [test -n $hello] (text if true) (command if false)
:loop 

[] # string
() # command

[] # 1 or more arguments
{} # 0 or more arguments

() # string
(this is just plain text, everything before the first null character is interpreted. nothing is evaluated besides a closing ")')'"
(just plain text how about embedded parantheses? you can embed parenthesis how about expansion operators? that would be useful too)
and how about embedding quotes? Do i even need quotes now?

(hello )[? city world]
(? ) (hello )[? city world]
(? ) hello city hello world

:if (test -z {$var}) (? a[bc aa]) (? c[ba cc])

`` `~
`( `)
`{ `}
`[ `]

(abc [ nothing ])

:if (:if (true) (echo hi) (echo bye))

(this is a set of parens: (), and this is a single paren: )`(
~hello(
)

="hello world"
echo (hello world ())

( () () ( ( ) ) only paren )

( ( ( [ { } ] ) ) )

? (hello world here is a paren `( and here is another paren `). the backtick "``" is the escape character. )

~EOS
hello world
EOS

backtick just takes the next character as raw. There is no way to continue onto
the next line unless there is a newline in the actual argument.





echo ( testing some text)

What works within parens?

Backtick?
Command expansion?

no, expansion doesn't make this easy

echo (this is some    great text. you just need a "`(" to escape a parethesis)

there needs to be a way to escape parens somehow.
` backtick
' quote
" quote

`(

Honestly, backtick is a pretty unused character, and looks like a backslash. So that's probably the best way to do it.

echo alan's words are cool



[? :cd /home echo hello | :f 1]

(i am a string)
echo (hello world) (This is another string) (

echo (
    it [$env] preserves whitespace
)


:if '' "echo hello"

:if true ~(
    :if false ~(
        echo hello world
        mktemp -d
    ) ~(echo this is executed)
) ~(
    echo death to all
    date
)

~(saoentu)~
~[saoentu]~
~{saoentu}~

$var     - local var command
%var     - env var command
:builtin - builtin command

~str     - heredoc command
?str     - join command
`cmd     - regular command, remove the backslash when run

only need to escape these characters: ( ) [ ] { } | ' "

$ - anonymous variable
% - anonymous variable

special variables:
9kk

$var : stable,   untouched by builtins, completely local to the script
%var : unstable, may be imported, builtins change some, may be exported
:var : unstable, read only, may be variable or command, usage depends on documentation

:.   : return/set the current directory

echo [:.] # history of all directories in session

$dirs_snapshot [:.]
ls [:.]/[? a b]
:. (%HOME)
:. /bob

: is the temporary system variable. used for arguments on startup and temporary variables in loops

$args (:)

ax.len ($args) | $len | :nil

$1 (:|.ind 1)
$2 (:|.ind 2)


":" is the anonymous argument
:forin %(env --keys) (
    (:) (:)" "(pwd)
)

:exec
    
%(env --keys)
env command

shortcut system:
fzf?



hook for every builtin?


%
:

:replace-with-env | ? 'HAPPY'
? 'HAPPY' | :replace-with-env

temporary env var:
    %var foo; cmd; %var ''
    ? foo | %var; cmd; %var unset
    %var foo; cmd; :del %var

%var foo; cmd; ?|%var
?|%var
%var ''

if there is standard in, use that.
if not, use arguments. arrays are supported.
if both, idk.
env var stdin
    ? foo | %bar

temporarily hide env var: 


$~ ($HOME)
$. ($..)

hooks:
    exit hook - called when you ":exit"
    cd ($~)
    cd ($..)
    cd ($.)

folder on path? that might be interesting. kinda serves as namespace.

What is the line between a programming language and a shell?
- a programming language works with some sort of machine instructions.
- a shell is the glue that makes it easier to string text based programs together.

Shell Program:
- A program that accepts an argument list (argv), environment variables, and standard in.

arg list, executable path, env variables, std in, files.

Shell Program Inputs:
- Current Directory
- Parent PID
- User
- Environment Variables
- File System
- Program Arguments
- Stdin
- External Things

Shell Program Outputs:
- Stdout
- Stderr
- Return Code
- File System
- External Things

stdin/out is both input and output.
files are both input and output.
stdin/out is just a special file.
arg list, ret code, env vars are special things.
arg list is a list. env vars are a map. ret code is a return value.

I don't really like that return codes are the only number available. It could
be empty string for 0 and error code for non zero error codes. That's kinda the
opposite of a normal boolean though.

Should there be first class support for files? I feel like that's really just
add-ons. Like being able to tab complete for files.

:if 

ret code, stdout, stderr, files.


A shell pro
A shell works with a program that accepts a list of arguments, environment
variables, and standard in.

A shell is both the glue that combines text based programs. Shells

## functions
functions are useful in scripts, but it means they can't be used outside the script...
also namespaces are a big problem...

namespace can be solved by just updating your path. that doesn't sound hard.

do you need to make a utility function for a script? Options:
- embed the function in the script
- separate the function into another script
- write the function in another language
- don't have a function

function is like storing the code in memory. it's a temporary script. it's technically impossible for sub commands to execute the script

there are local variables and env variables.

why not local (function) and external commands? Commands are files. Stored on the disk. So it's a bit different. Best is to just require separate files. That's more efficient than making a file at startup. It's so easy to create a function/alias system. And so hard to design without one.

You could just assume a specific directory isn't in your path, then add that directory to your path for this program. Also technically an external program can access your program from the file system. So that whole function thing doesn't really help. But local variables help declutter the environment variables. And keeps it somewhat more secure. But the PATH gets cluttered with bunches o aliases. So what. That's interactive. The language is meant for interactive use. You don't have to write scripts in ax. It's good at combining programs.


That's a good question.



Also it's annoying when not all your shells get an alias update. They could share the same path to fix that. That allows flexibility.

And to make functions specific for your program, you could just define them at
the top. There could be a separate command for storing stdin into a file. and add that to your path

is there a way to figure out which executables are needed for a script?

## reference
Help on implementing pipes: https://stackoverflow.com/questions/13636252/c-minishell-adding-pipelines/

eof is not a character. it's the status of the file descriptor. eof is hit, then sigpipe. you would need to ignore sigpipe for it not to affect your process. 

read on coprocs (named pipes kinda)
https://newbedev.com/how-do-you-use-the-command-coproc-in-various-shells

## handling ctrl+z (interrupt)
ctrl+z sends the sigtstp (terminal stop) signal to the process. that process doesn't then is stopped and doesn't resume until a sigcont. this means the ax shell probably needs to handle this.

another thing, i want only one thing to be able to attach itself to stdout at one time. background processes can't write 

sigstop cannot be ignored. sigtstp can be ignored. both are resumed with sigcont.

the ampersand runs in the background. when you close your shell, those tasks are stopped too.

the "nohup" command can be used to run a command in the background. output can be sent to a file.

since only one thing can print to the screen at one time. i don't want parallel commands to print to the screen. they can print to /dev/null, unless you manually are redirecting output to a file. or you bring the process to the foreground.

read on job control: https://unix.stackexchange.com/questions/509188/does-a-shell-which-does-not-support-job-control-has-the-concept-of-foreground-an

the ampersand says it should be run in the background, but it is still connected to the terminal. if you want something completely not connected to the terminal, maybe start it with "setsid(1)".

Sooo.. now what? job control makes everything confusing. I wish I could survive without it. I wish it was handled somewhere else. When you ctrl+z, sigtstp is sent, the shell could just immediately send sigcont, so stopped processes are not supported. You'd need to kill it.

A command could be used to start a process in the background. It doesn't have to be a builtin.

DONE: Study up on this: https://en.wikipedia.org/wiki/Process_group
TODO: Study on this: https://www.win.tue.nl/~aeb/linux/lk/lk-10.html

    One may use the command ps j to see PPID (parent process ID), PID (process
    ID), PGID (process group ID) and SID (session ID) of processes. With a shell
    that does not know about job control, like ash, each of its children will be in
    the same session and have the same process group as the shell. With a shell
    that knows about job control, like bash, the processes of one pipeline. like

    % cat paper | ideal | pic | tbl | eqn | ditroff > out
    form a single process group.

Processes can change their own group. And the parent of a process can change the process group of children. No other process can change a process group.

Ok. In a session, only one process group can be active and therefore write to stdout or read from stdin. What if the shell handled multiple process groups, but did not attach any of those process groups to sessions? Maybe a separate command could attach it to a session (if possible, prob not)?

A process group is "orphaned" when the leader terminates. The leader is the first process of the group.

This is weird functionality:
    If termination of a process causes a process group to become orphaned, and some member is stopped, then all are sent first SIGHUP and then SIGCONT.

Sighup means the terminal was disconnected. It could be sent by a physical terminal being disconnected or emulated terminal being disconnected.

The kernel does not allow background process groups 

If the parent process dies, the parent pid is now init (aka systemd, or #1).

All signals except sigkill and sigstop can be caught/ignored/blocked.

It's convention for a PGID to be the same number as the PID of the first member of the group.

This will show the PID, PPID, PGID, and PSID:

    ps j

What is TPGID?

There are 4 signals: PID, PPID, PGID, SID
- Process         ID: id of the process
- Process Parent  ID: id of the process that spawned this process
- Process Group   ID: used for signals, signal sent to group is sent to all processes (defaults to parent's process group)
- Process Session ID: used for shells, if shell dies, these die

pid, pgid, and psid are sometimes all the same.

You can set your "PSID".

PID     PPID    PGID    PSID
2818610 2818603 2818610 2818610 zsh              -- PID = PGID = PSID         -- makes sense. 
2818615 2818610 2818615 2818610 startx           -- PID = PGID & PPID = PSID  -- i'm in a session (if shell dies i die) and i have sub-groups
 867110       1  867108  819515 sxhkd            -- 
 961755  961258  961223  961223 QtWebEngineProc  -- PGID = PSID
      1       0       1       1 systemd          -- only two things (probably systemd) has a parent id of 0.
      2       0       0       0 kthreadd         -- these would have started at boot time
    258       2       0       0 card0-crtc2      -- every time PGID is 0, PSID is always also zero (empirically)

I really don't want job control. I need to put all things in a pipe into a single process group though. 

I should make this use case easy, might require a separate command though:
- run multiple commands in parallel and write the output of each to the same file.

Not sure if possible, but I did just think of an ideal (like a week later).

Only one process group is active at any time. If you "ctrl-z" on something, the
kernel will send a stop signal, then the will see that the process is alive and
send a hangup signal, meaning the terminal has been disconnected from the
process. If the process is still alive, maybe send a forceful kill signal.

If you want to be able to stop a process and resume it, maybe there could be
another command for that. It would basically keep track of the pdid in a file.
Not sure exactly how it would work. Maybe change the session on the process to
something else unless the process is resumed.

I really don't like the idea of a "paused" process. I also don't like the idea
of multiple processes writing to the same output. And I also don't like the
idea of having 2 output streams (stderr and stdout).

What is my opinion on return codes? in programming languages, there are
exceptions and returns. Something like a daemon can't depend on stdout, but it
can depend on the return code. Is there a use case for both capturing stdout
and return code.

This is really clean:
() -- out/array
{builtin}

echo hello | !split ($var) (tee file)
echo hello | !out hi

!out (echo hi; echo bye | first) | $var
!if (echo test | !ret) (echo hi)

null (Fe

ind 0, -1, 

echo hello (echo hello world|!1)

blah | !out # default
blah | !err # drops stdout & prints stderr in realtime
blah | !oe  # out and err to out
blah | !ret | $var ''

+err  | !err | !err
blah | !out | !err | !


$var -- stdout
blah | $var -- stderr
blah | $var -- return

$var x y z # do these have any difference?
!var x y z

$ hello world # out
$var kkkk
! 

if (blah | !ret)

echo hello | !if 

| !err varname | !out varname

!if [command to generate conditions] # does array make sense with if?


$ret (blah)
!if {return value} ()

$rets (!ret)

!get (command) err varname ret varname
!if () (looks at return codes)

ignore err & ret


{.} -- stdout of previous process group
{!} -- stderr of previous process group
{?} -- return codes of previous process group

!out (echo hello) {.} | hello
echo {.}
!out {.}

(

9kk

!capture (ai ya) out err ret

()
(<{}>)

(echo hi; echo yo; echo no; yoyo | cat) # rets: 

($var)

() output of last
[] err of last
{} var

!if {out}

!(do the command) out

!err
!out
!ret

()  -- nothing
1() -- out
2() -- err
12() -- out and err

*^() -- ret arr

*|  -- pipe stdout and stderr
!builtin
 !  -- 
^%
^'
^"
^#  -- rest of line is comment ignore null character

are semicolons needed? (only for single line things, but how often does that happen?

echo {echo hi . echo bye} {



Things that change the meaning of the next character:

* change (array)
Above things: *^`'"
# *'" are set
^`
# `[] = string (include [ and ]) 
# *[] = array

echo `([echo hi] (echo yo) # (hi 0
echo `(echo hello) # (echo hello)

echo x*{!out `{{echo hello}} `]a b c} # xa xb xc

$'var hello' [echo hello world]
$error <echo error>

echo <<echo>> # prints "echo"
echo <<echo>> # prints

echo {^{echo echo}^} # prints "{echo}"
echo 5 % 2  i`{{echo echo}`} # prints "{echo}" `

echo `
hello`
world`
% this is the end. # \nhello\nworld\n this is the end.

stdout to stderr # can be done with an external program
stderr to stdout # can be done with <>
stderr and stdout # If there were a fifth bracket, that could be good...
pipe doesn't need any weird mode

!allow-stderr

Maybe get rid of return code.
!out *{err} errcho hello | cat

!set err t
!set out f

*{ret}

noerr # ignores stderr

echo hello | !err # out to err
echo hello | !nil # out to err
echo hello | !out # noop

!err {}

*^jj
{var}
(extract output)


Things that happen afterwards is weird. Capturing multiple things is weird (can it be made normal?).
Implicit variables is weird.

One line should not affect the other line at all unless env vars are set

$var1 err out ret err-and-out

!run (cmd)

echo hello # echos to stdout, stderr, 

!out
!nil
!err

!{ret}
![] hi

is it annoying to see stderr in the output?kkk

!{

{
*% testing testing

<()>
{PATH}
{{ 
{stdout} [stderr] (ret)
`var`
<var>

!idk

!{}
!<>
!()
!

a way to filter env vars right before they're passed to a function/pipeline.

a thing. that filter could be used with path.

!{} echo hello world | !split {cat} {cat:echo:echo} {cat:echo}

| !ret

## handling stderr
extra pipe character?
capture all stderr into buffer and have builtin to show buffer output?

first rule. the screen only has one view, so you can only output to standard out.

but programs can output to out or err. err is dropped by default.

| = out & err
|out
|err
|hello

(0 = stdin, 1 = stdout, 2 = stderr, 3 = specific)

wherever i'm going with this, pipes would be required to be their own word. also, pipes other than out and err 

merging pipes makes sense. but what about splitting pipes? splitting a pipe is probably useful, but how could it function?

## how do you make an alias for an existing command
example in bash:

```
alias ls="ls --color"
```

AX_BIN=/tmp/ax-1234
PATH=/tmp/ax-1234:/usr/bin

## nested ax shells and AX_BIN
if you nest ax shells, 

## create process in background
there are two meanings of "process in background":
* run in background but kill when shell exits
* run in background and keep alive when shell exits

maybe the shell could just support the first meaning. an external command could provide the ability to do the second.

```
daemonize cmd arg1 arg2...
```

actually, it may be possible for both to be supported in an external command. maybe... maybe not though. i was thinking of an option for the "daemonize" program to listen for it's parent process and killing the sub process if the parent process is killed. but that also means that the daemonize program needs to continue running. so it would need to spawn 2 threads instead of just one. this is getting more complex, so it may not be an option.

## key bindings
are keybindings just a way to run commands and put the output where the cursor is? no. keybindings can be used to manipulate any text in the edit buffer. wish there was a keybinding standard... is it possible to separate the keybinding logic into a separate program?

should the current buffer be in an environment variable?

better definition... keybindings need to be able to control both the cursor and the input buffer. so you execute filters on the buffer. one idea:

stdin is the buffer. the null character is the cursor.
stdout is the new buffer. the null character again is the cursor.

with that approach, you could have multiple cursors. not sure if that's good or bad... history might be an array and that can't accept null, so cursor positions are not preserved. they weren't preserved before though, so that's fine.

you cannot have an "active" multiple cursor. you can just have multiple cursors.

some defaults:
* letters show letters after the cursor(s)
* shift+enter (newline)
* enter (execute)

## reserved characters
here is a list of all reserved characters. there is no escaping supported. instead of escaping, you should use one of the many string options.

```
builtins, variables, pipes, statements, comments:
*  -- modify reserved character to behave different
!  -- builtin
$  -- create var/array
|  -- pipe stdout
;  -- separate statements
#  -- rest of line is comment ignore null character

parameter strings:
`  -- escapes next character
^  -- here doc string (ignore null character)
%  -- rest of line is string (ignore null character)
'' -- string (ignore null character)
"" -- string (ignore null character)

command substitutions:
 *  -- modify transform one of the captures below to an array
[ ] -- exec & capture stdout (ignore stderr & return)
< > -- exec & capture stderr (ignore stdout & return)
( ) -- exec & capture return (ignore stdout & stderr)
{ } -- capture variable
```

## builtins
builtins must always be prepended with a "!". it's easy to tell whether something is an executable or just a builtin with this required prefix. unlike normal commands, builtins can:

* conditionally parse command substitutions
* receive stderr, and stdout through pipes

this allows for if and loop constructs to be represented as builtins, as well as redirecting stderr and stdout. there may be more builtins in the future, but the list of builtins will remain fixed at some point and cannot be extended by ax shell users.

```
!out    [arg]...            -- print args to stdout, separating with null character
!err    [arg]...            -- print args to stderr
!ret    [arg]...            -- return return codes
!cd     [dir]               -- change into a directory
!exit   [err]               -- leave the shell, returning optional error code
!if     [test expr]... expr -- conditional, then is printed to stdout
!loop   [test expr]... expr -- conditional loop, thens are printed to stdout
```

## env vars
the ax shell doesn't distinguish exported variables with local variables. only exported environment variables are supported. here are a few of the environment variables that the ax shell populates for you:

```
{AX_BIN}    -- the temporary directory that is prepended to the PATH variable
```

## examples
as of now, that's the complete syntax for the ax shell. though the list of builtins is not complete and the syntax is still subject to change, the shell is already very powerful as these examples will show.

```
# cd into /tmp/dir:
!cd /tmp/dir

# print the text "hello world" with no trailing newline:
!out 'hello world'

# or use an external program to do the same thing:
echo -n 'hello world'

# instead of aliases or functions, add to the path like this:
!out "#!/bin/ax" "!out {2}" | tr \0 \n | tee {AX_BIN}/print-second

# here is how you could make a shortcut for the above alias snippet using standard posix shell commands:

tee {AX_BIN}/alias | sed -E s/^ +//g | ? ~EOF
    #!/bin/ax
    $AX_BIN (tr -d \n | awk -F: '{print $2}' | !out {PATH})
    tee {AX_BIN}/{1} | tr \0 \n | ? (~ #!/bin/ax ~) "? {2}"
    !exit 0
EOF~

# and using the above shortcut, you could end up creating aliases like this:
alias print-second %!out {2}

# if we had a few imaginary shell commands available, you can imagine the alias shortcut to be a bit more elegant:
!out ^EOF
    #!/bin/ax
    $AX_BIN [range 2 *[!out {PATH} | tr : \0]]
    !out "#!/bin/ax" "!out {2}" | tr \0 \n | tee {AX_BIN}/{1}
EOF | trim | tee {AX_BIN}/alias

# here is what a conditional might look like. test is not a builtin, it's a standard unix command:
!if (test -n {VAR}) [echo var is non-empty] [echo var is empty]

# in this example, the program will sleep for 1 second until the variable is set:
!loop (test -z {VAR}) [sleep 1s]

# here is an if elseif statement:
!if (test -n {V1}) [echo V1 is non-empty]
    (test -n {V2}) [echo V2 is non-empty but V1 is empty]
                   [echo both V1 and V2 are empty]

# there is no globbing in ax. instead of "ls */*" like in bash, use a command to get the argument list:
ls *[find -print0 -mindepth 2 -maxdepth 2 -not -path '*/.*']

# or if you have "fd", the above syntax is much more sane to work with:
ls *[fd -0I --exact-depth 2]
```

## conclusion
some things i can think of right now that still need to be hashed out are:
* process handling
* what to do with standard error
* finalizing the list of builtins

i need to spend some time today doing something else rather than working on this blogpost now. you can see that even with a basic syntax, a powerful shell can be constructed. email me at alan@xoc3.io if you have questions or feedback about this. i'll start prototyping this shell when i consider the design to be complete, which will probably not be for many months at least.
